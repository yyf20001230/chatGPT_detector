# -*- coding: utf-8 -*-
"""NLPArticleTransformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XOyqNh8rJF_nqs-WqiD0e1awOyox15dy
"""

# Different layers
from tensorflow.keras.layers import MultiHeadAttention, Input, Dense
from tensorflow.keras.layers import LayerNormalization, Layer
from tensorflow.keras.layers import TextVectorization, Embedding, GlobalAveragePooling1D
# For miscellaneous functions
from tensorflow.data import Dataset
from tensorflow import convert_to_tensor, string, float32, shape, range, reshape, keras
from tensorflow.keras import utils
# Keras models
from tensorflow.keras import Model, Sequential
# For datasets
import pandas as pd
# For evaluation
from sklearn import metrics
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
# For math/arrays
import numpy as np
# For plotting
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')
#drive.mount("/content/drive", force_remount=True)

# Import data from Excel due to the csv having commas in the data
_data_big = pd.read_excel("/content/drive/MyDrive/InputData.xlsx")

# Remove the first col due to it being extraneous data
_data_big = _data_big.drop(['Unnamed: 0'], axis=1)

_data_big

# Some other config
_label_names = ['wiki', 'gpt']

_data_big = _data_big.iloc[:1000, :]

# Now let's remove any weird symbols (like "â‰ˆ" or "~")
for row in _data_big.iloc:
  row[0] = ''.join(ch for ch in row[0] if (ch.isalnum() or ch == ' '))
  row[1] = ''.join(ch for ch in row[0] if (ch.isalnum() or ch == ' '))

# Randomize order and make train/test split
_ratio = 0.8 # 0.8 means 80% of data -> training
_number_rows = _data_big.shape[0]
_amount_training = int(_number_rows * _ratio)
_amount_test = _number_rows - _amount_training

_shuffled = _data_big.sample(frac=1) # Shuffle order of rows
_training_data = _shuffled.iloc[0:_amount_training]
_test_data = _shuffled.iloc[_amount_training:]

# Now let's store all of this data into train_X/Y and test_X/Y
# Note that 0 = wiki, 1 = GPT generated
all_X = []
train_X, train_Y = [], []
test_X, test_Y = [], []

for row in _training_data.iloc:
  all_X.append(row[0])
  all_X.append(row[1])

  train_X.append(row[0])
  train_Y.append(0)
  train_X.append(row[1])
  train_Y.append(1)

for row in _test_data.iloc:
  all_X.append(row[0])
  all_X.append(row[1])

  test_X.append(row[0])
  test_Y.append(0)
  test_X.append(row[1])
  test_Y.append(1)

len(test_X)

len(test_Y)

"""Below is taken directly from the tutorial"""

# Taken directly from tutorial

# Now let's vectorize our strings
vocab_size = 5000 # The total distinct words to use
sequence_length = 25 # Max sequence length

train_X_tensor = Dataset.from_tensor_slices(all_X)

# TextVectorization layer
vectorize_layer = TextVectorization(output_sequence_length=sequence_length, max_tokens=vocab_size)

# Adapt method trains the TextVectorization layer and creates a dictionary
vectorize_layer.adapt(train_X_tensor)

# Vectorize all strings
train_X_tensors = convert_to_tensor(train_X, dtype=string)
train_X_vectorized = vectorize_layer(train_X_tensors)
test_X_tensors = convert_to_tensor(test_X, dtype=string)
test_X_vectorized = vectorize_layer(test_X_tensors)

train_Y_categorical = utils.to_categorical(train_Y) # [0, 0, 1, 0] --> [0 - is it true, 1 - is it true] [[1, 0], [1, 0], [0, 1], [1, 0]]
test_Y_categorical = utils.to_categorical(test_Y)

class EmbeddingLayer(Layer):
  def __init__(self, sequence_length, vocab_size, embed_dim):
    super(EmbeddingLayer, self).__init__()
    self.word_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)
    self.position_embedding = Embedding(input_dim=sequence_length, output_dim=embed_dim)

  def call(self, tokens):
    sequence_length = shape(tokens)[-1]
    all_positions = range(start=0, limit=sequence_length, delta=1)
    positions_encoding = self.position_embedding(all_positions)
    #tf.print(positions_encoding)
    #tf.make_ndarray(positions_encoding)

    words_encoding = self.word_embedding(tokens)

    #tf.print("WORDS: ", words_encoding, "\nPOSITIONS: ", positions_encoding)

    return positions_encoding + words_encoding

class EncoderLayer(Layer):
  def __init__(self, total_heads, total_dense_units, embed_dim):
    super(EncoderLayer, self).__init__() # Multihead attention layer
    self.multihead = MultiHeadAttention(num_heads=total_heads, key_dim=embed_dim) # Feed forward network layer
    self.nnw = Sequential([Dense(total_dense_units, activation="relu"), Dense(embed_dim)]) # Normalization
    self.normalize_layer = LayerNormalization()

  def call(self, inputs):
    attn_output = self.multihead(inputs, inputs)
    normalize_attn = self.normalize_layer(inputs + attn_output)
    nnw_output = self.nnw(normalize_attn)
    final_output = self.normalize_layer(normalize_attn + nnw_output)
    return final_output

import tensorflow as tf
tf.config.run_functions_eagerly(True)

class PrintLayer(Layer):
  def __init__(self):
    super(PrintLayer, self).__init__()

  def call(self, inputs):
    tf.print(inputs)
    return inputs

embed_dim = 100
num_heads = 2
total_dense_units = 20
# Our two custom layers
embedding_layer = EmbeddingLayer(sequence_length, vocab_size, embed_dim)
encoder_layer = EncoderLayer(num_heads, total_dense_units, embed_dim)
LSTM_layer = tf.keras.layers.LSTM(16)

# Start connecting the layers together
inputs = Input(shape=(sequence_length, 1))
lstm = LSTM_layer(inputs)
d1 = Dense(total_dense_units, activation="relu")(lstm)
d2 = Dense(total_dense_units * 4, activation="relu")(d1)
d3 = Dense(total_dense_units * 8, activation="relu")(d2)
d4 = Dense(total_dense_units * 8, activation="relu")(d3)
d5 = Dense(total_dense_units * 4, activation="relu")(d4)
d6 = Dense(total_dense_units * 2, activation="relu")(d5)
d7 = Dense(total_dense_units * 0.5, activation="relu")(d6)
d8 = Dense(total_dense_units * 0.25, activation="relu")(d7)
#emb = embedding_layer(lstm)
#enc = encoder_layer(emb)
#pool = GlobalAveragePooling1D()(enc)
#d = Dense(total_dense_units, activation="relu")(pool)
outputs = Dense(2, activation="softmax")(d8) # 2 = number of outputs

# Construct the transformer model
transformer_model = Model(inputs = inputs, outputs = outputs)
transformer_model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.0005), loss = "binary_crossentropy", metrics = ['accuracy'], run_eagerly = True)
transformer_model.summary()

history = transformer_model.fit(train_X_vectorized, train_Y_categorical, batch_size=24, epochs=4, validation_split=0.33)

fig = plt.figure(figsize=(15,4))
metric = ['loss', 'accuracy']
validation_metric = ['val_loss', 'val_accuracy']

for i,j,k in zip(metric, validation_metric, np.arange(len(metric))):
  fig.add_subplot(121+k)

  plt.plot(history.history[i])
  plt.plot(history.history[j])
  plt.legend(['Training', 'Validation'])
  plt.title('Training and validation ' + i)
  plt.xlabel('Epoch Number')
  plt.ylabel(i)

# Get the random embeddings
random_embedding_layer = EmbeddingLayer(sequence_length, vocab_size, embed_dim)
random_emb = random_embedding_layer(inputs)
random_embeddings_model = Model(inputs=inputs, outputs=random_emb)
random_embedding = random_embeddings_model.predict(train_X_vectorized[0:1,])
random_matrix = reshape(random_embedding[0, :, :], (sequence_length, embed_dim))

# Get the learned embeddings
learned_embeddings_model = Model(inputs=inputs, outputs=emb)
learned_embedding = learned_embeddings_model.predict(train_X_vectorized[0:1,])
learned_matrix = reshape(learned_embedding[0, :, :], (sequence_length, embed_dim))

# Render random embeddings
fig = plt.figure(figsize=(15, 15))
ax = plt.subplot(1, 2, 1)
cax = ax.matshow(random_matrix)
plt.gcf().colorbar(cax)
plt.title('Random embeddings', y=1)
ax.set_xticks(())

# Render learned embeddings
ax = plt.subplot(1, 2, 2)
cax = ax.matshow(learned_matrix)
plt.gcf().colorbar(cax)
plt.title('Learned embeddings', y=1)
ax.set_xticks(())

# For confusion matrix
test_predict = transformer_model.predict(test_X_vectorized)
test_predict_labels = np.argmax(test_predict, axis=1)

fig, ax = plt.subplots(figsize=(15, 15))

# Create and display the confusion matrix
test_confusion_matrix = confusion_matrix(test_Y, test_predict_labels)
cm = ConfusionMatrixDisplay(confusion_matrix=test_confusion_matrix,
display_labels=_label_names)
cm.plot(xticks_rotation="vertical", ax=ax)
plt.title('Confusion Matrix of the Test Set')
plt.show()
print('Correct classification: ', np.sum(np.diagonal(test_confusion_matrix)), '/', len(test_predict_labels))

